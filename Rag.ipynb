{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87c3dd4f",
   "metadata": {},
   "source": [
    "**Reference Link**: https://github.com/DataTalksClub/llm-zoomcamp/tree/main/01-intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fc92659-c84d-41ab-82ef-c7e36297d92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2494e136",
   "metadata": {},
   "source": [
    "### Reading the faq llm zoomcamp file which is in json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9a7420a-149f-45aa-8ad0-ffa3186739db",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('documents-llm.json', 'rt') as f_in:\n",
    "    docs_raw = json.load(f_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645eddf8",
   "metadata": {},
   "source": [
    "We are adding the course inside the documents which contains text, question and answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d37d4211",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for course_dict in docs_raw:\n",
    "    for doc in course_dict['documents']:\n",
    "        doc['course'] = course_dict['course']\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62bd00a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Yes, but if you want to receive a certificate, you need to submit your project while we‚Äôre still accepting submissions.',\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'I just discovered the course. Can I still join?',\n",
       " 'course': 'llm-zoomcamp'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da09eb62",
   "metadata": {},
   "source": [
    "We are trying to do RAG implementation using Elastic search.\n",
    "\n",
    "### üß† RAG (Retrieval-Augmented Generation)\n",
    "**RAG** is a technique used in natural language processing (NLP) to improve the quality of generated text by retrieving relevant documents before generating a response.\n",
    "\n",
    "### üîç How RAG Works:\n",
    " **Retrieval Phase:**\n",
    "- A query is sent to a document store (like a vector database).\n",
    "- The system retrieves relevant documents or passages based on semantic similarity.\n",
    "\n",
    "**Generation Phase:**\n",
    "- A language model (like GPT) uses the retrieved documents as context.\n",
    "- It generates a response that‚Äôs grounded in the retrieved information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53063695",
   "metadata": {},
   "source": [
    "#### üîé Elasticsearch\n",
    "**Elasticsearch** is a search engine based on Lucene, designed for fast and scalable full-text search.\n",
    "\n",
    "**‚öôÔ∏è Key Features:**\n",
    "- Indexing: Stores data in a structured format for fast retrieval.\n",
    "- Search: Supports keyword search, fuzzy search, and filtering.\n",
    "- Analytics: Can perform aggregations and visualizations (often used with Kibana).\n",
    "\n",
    "**üß† How It Works:**\n",
    "- Data is stored in JSON documents.\n",
    "- You can query using a powerful DSL (Domain Specific Language).\n",
    "- It‚Äôs optimized for text search, log analysis, and real-time data exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f86546",
   "metadata": {},
   "source": [
    "To run elastic search using Docker, run the following command in command line/terminal\n",
    "\n",
    "docker run -it \\\n",
    "    --rm \\\n",
    "    --name elasticsearch \\\n",
    "    -m 4GB \\\n",
    "    -p 9200:9200 \\\n",
    "    -p 9300:9300 \\\n",
    "    -e \"discovery.type=single-node\" \\\n",
    "    -e \"xpack.security.enabled=false\" \\\n",
    "    docker.elastic.co/elasticsearch/elasticsearch:8.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26c70f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e5e6faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_client = Elasticsearch('http://localhost:9200') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc30ec19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created index: course-questions\n"
     ]
    }
   ],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    }, \n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"} \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\"\"\" number_of_shards = 1\n",
    "\n",
    "Shards are like splitting your folder into smaller subfolders so Elasticsearch can search faster.\n",
    "\n",
    "Here, 1 means we‚Äôre keeping everything in a single shard (good for small datasets).\n",
    "\n",
    "number_of_replicas = 0\n",
    "\n",
    "Replicas are backup copies of your shards for fault tolerance.\n",
    "\n",
    "Here, 0 means no backups ‚Äî fine for testing, but risky for production.\n",
    "\n",
    "Mapping part defines the structure of the data in the index ‚Äî like setting column types in a database.\n",
    "\n",
    "- text, section, question ‚Üí type: \"text\"\n",
    "- These fields will be analyzed for full-text search.\n",
    "- Elasticsearch will tokenize and index them for efficient matching.\n",
    "\n",
    "- course ‚Üí type: \"keyword\"\n",
    "- This field is not analyzed.\n",
    "- Used for exact matches, filtering, and aggregations (e.g., grouping by course name).\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "index_name = \"course-questions\"\n",
    "\n",
    "# If the index already exists, skip creation to avoid accidental deletion of data.\n",
    "try:\n",
    "    if es_client.indices.exists(index=index_name):\n",
    "        print(f\"Index already exists: {index_name} - skipping creation.\")\n",
    "    else:\n",
    "        es_client.indices.create(index=index_name, body=index_settings)\n",
    "        print(f\"Created index: {index_name}\")\n",
    "except Exception as e:\n",
    "    print('Warning: failed to check/create index:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6d5703f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Yes, but if you want to receive a certificate, you need to submit your project while we‚Äôre still accepting submissions.',\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'I just discovered the course. Can I still join?',\n",
       " 'course': 'llm-zoomcamp'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e8c7684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc499dcf",
   "metadata": {},
   "source": [
    "Code is inserting documents into your \"course-questions\" index in Elasticsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5e9e6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1480908d62466fa326e0a7515f1ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for doc in tqdm(documents):\n",
    "    es_client.index(index=index_name, document=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99160d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"how to get access to saturn cloud\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f62902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_search(query):\n",
    "    search_query = {\n",
    "        \"size\": 5, #This means: only return 5 documents (results).\n",
    "        \"query\": {\n",
    "            \"bool\": { #A bool query in Elasticsearch lets you combine conditions ‚Äî like saying must match this AND must pass that filter.\n",
    "                \"must\": {\n",
    "                    \"multi_match\": { #multi_match = Search for the same query across multiple fields.\n",
    "                        \"query\": query,\n",
    "                        \"fields\": [\"question^3\", \"text\", \"section\"], #\"question^3\" ‚Üí The ^3 means boost the importance of matches in question by 3√ó., also search in text and section fields.\n",
    "                        \"type\": \"best_fields\" #Choose the single best field match for scoring.\n",
    "                    }\n",
    "                },\n",
    "                \"filter\": { #Filters are used to narrow down the search results.\n",
    "                    \"term\": { #term = Search for a term in a specific field.\n",
    "                        \"course\": \"llm-zoomcamp\" #Search for the term \"llm-zoomcamp\" in the \"course\" field. \n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = es_client.search(index=index_name, body=search_query)\n",
    "    \n",
    "    result_docs = []\n",
    "    \n",
    "    for hit in response['hits']['hits']: #Loop through the hits (results) and extract the relevant information.\n",
    "        result_docs.append(hit['_source']) #Append the source of the hit (the document) to the result_docs list.\n",
    "    \n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6998b28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Please see the General section or use CTRL+F to search this doc.',\n",
       "  'section': 'Module 2: Open-Source LLMs',\n",
       "  'question': 'Saturn Cloud issues',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Issue: I get the notice that due to traffic, I‚Äôm on a waitlist for new signups.\\nAnswer: There was a form to submit our emails to, so Alexey can send it in bulk. If you missed that deadline, just sign up manually (or via request tech demo link) and use the chat to request for free hours for ‚Äúllm zoomcamp‚Äù\\nIssue: I‚Äôm a pre-existing user from a different zoomcamp and I‚Äôm not awarded the free hours even though I‚Äôve submitted my email in the form.\\nAnswer: Just request it via their chat, after you‚Äôve logged in using your pre-existing account, citing ‚Äúllm zoomcamp‚Äù .',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'SaturnCloud - How do I get access?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Manually set the token as below:\\naccess_token = <your_token>\\nmodel  = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", token=access_token)\\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", token=access_token)',\n",
       "  'section': 'Module 2: Open-Source LLMs',\n",
       "  'question': 'Mistral AI: Unable to get Mistral-7B-v0.1 access despite accepting terms on HF',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': 'Clean out your cache using the following code:\\nfrom transformers import TRANSFORMERS_CACHE\\nprint(TRANSFORMERS_CACHE)\\nimport shutil\\nshutil.rmtree(TRANSFORMERS_CACHE)\\nNote: Make sure to shutdown the notebook and restart the kernel',\n",
       "  'section': 'Module 2: Open-Source LLMs',\n",
       "  'question': 'SaturnCloud: How can I clean out the hugging face model cache on a saturn cloud notebook?',\n",
       "  'course': 'llm-zoomcamp'},\n",
       " {'text': '1. search with the model name on hugging face.\\n2. get the transformer used on the model.\\n3. using the transformer, encode the string you want.\\n4. calculate the length of the outputted tensor.\\nThe previous code snippet uses the tokenizer of google/gemma-2b LLM. \\nDon‚Äôt forget to make your token secret.\\nAdded by kamal',\n",
       "  'section': 'Module 2: Open-Source LLMs',\n",
       "  'question': 'HuggingFace: How to get the number of tokens in a certain string related to a certain model on hugging face?',\n",
       "  'course': 'llm-zoomcamp'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results = elastic_search(query)\n",
    "search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e165142b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "## Libraries Required\n",
    "%pip install langchain-huggingface --quiet\n",
    "## For API Calls\n",
    "%pip install huggingface_hub --quiet\n",
    "%pip install transformers --quiet\n",
    "%pip install accelerate --quiet\n",
    "%pip install  bitsandbytes --quiet\n",
    "%pip install langchain --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "032dba28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dotenv in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.9.9)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/python/3.12.1/lib/python3.12/site-packages (from dotenv) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "%pip install dotenv \n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f7b5518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f0c446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "key=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b9709b",
   "metadata": {},
   "source": [
    "Calling LLM from hugging face model hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1790fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\n",
    "def llm(prompt):\n",
    "    client = InferenceClient(\n",
    "        provider=\"groq\",\n",
    "        api_key=key,\n",
    "    )\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"openai/gpt-oss-120b\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a36e7b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\" \n",
    "        \n",
    "    \"\"\"Loops through each doc from Elasticsearch search results.\n",
    "\n",
    "        For each document:\n",
    "\n",
    "        Adds the section name.\n",
    "\n",
    "        Adds the FAQ question from the DB.\n",
    "\n",
    "        Adds the answer (stored in text).\n",
    "\n",
    "        Each entry is separated by a blank line for readability.\"\"\"\n",
    "            \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3199895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    search_results = elastic_search(query) #getting the relevant results from elastic search\n",
    "    prompt = build_prompt(query, search_results) #passing the results with query and prompt in a proper format to llm\n",
    "    answer = llm(prompt) #getting the answer from hugging face llm\n",
    "\n",
    "    # Extract the raw text from retrieved docs for RAGAS\n",
    "    contexts = [doc[\"text\"] for doc in search_results]\n",
    "    return answer, contexts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4f84e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: **How to get access to Saturn‚ÄØCloud for the LLM Zoomcamp**\n",
      "\n",
      "1. **If you see a ‚Äúwait‚Äëlist due to traffic‚Äù notice**  \n",
      "   - There was a form where you could submit your email so the instructors (Alexey) could add you in bulk.  \n",
      "   - **If you missed that deadline:**  \n",
      "     - Sign up directly on Saturn‚ÄØCloud (or use the ‚Äúrequest tech demo‚Äù link).  \n",
      "     - After you‚Äôve created the account, open the Saturn‚ÄØCloud chat and **request free hours for ‚Äúllm zoomcamp.‚Äù**\n",
      "\n",
      "2. **If you already have a Saturn‚ÄØCloud account from another Zoomcamp**  \n",
      "   - Log in with your existing account.  \n",
      "   - In the Saturn‚ÄØCloud chat, **request the free hours again, explicitly mentioning ‚Äúllm zoomcamp.‚Äù** This will attach the appropriate credits to your account.\n",
      "\n",
      "In short: submit your email via the course form (or sign up manually), then use the Saturn‚ÄØCloud chat to ask for the ‚Äúllm zoomcamp‚Äù free‚Äëhours allocation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query=\"how to get access to saturn cloud\"\n",
    "answer, contexts = rag(query)\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2194fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Scores: {'faithfulness': 5, 'relevancy': 5, 'precision': 2, 'recall': 4, 'bias': 5, 'hallucination': 5}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# Your existing LLM client\n",
    "client = InferenceClient(provider=\"groq\", api_key=key)\n",
    "\n",
    "def llm(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"openai/gpt-oss-120b\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "# Custom evaluation prompt\n",
    "def build_eval_prompt(query, answer, contexts):\n",
    "    return f\"\"\"\n",
    "You are an evaluator for a RAG-based chatbot. \n",
    "Evaluate the chatbot's answer on the following metrics. \n",
    "Give each metric a score between 1 (poor) and 5 (excellent). \n",
    "Return the result strictly as JSON with keys: faithfulness, relevancy, precision, recall, bias, hallucination.\n",
    "\n",
    "Definitions:\n",
    "- Faithfulness: Is the answer grounded in the provided context?\n",
    "- Relevancy: Does the answer address the user query?\n",
    "- Precision: Are the retrieved contexts relevant to the query?\n",
    "- Recall: Do the contexts cover all necessary information?\n",
    "- Bias: Is the answer neutral and unbiased?\n",
    "- Hallucination: Does the answer avoid unsupported or invented facts?\n",
    "\n",
    "User Query: {query}\n",
    "\n",
    "Chatbot Answer: {answer}\n",
    "\n",
    "Retrieved Contexts: {contexts}\n",
    "\n",
    "Output JSON format:\n",
    "{{\n",
    "  \"faithfulness\": <score 1-5>,\n",
    "  \"relevancy\": <score 1-5>,\n",
    "  \"precision\": <score 1-5>,\n",
    "  \"recall\": <score 1-5>,\n",
    "  \"bias\": <score 1-5>,\n",
    "  \"hallucination\": <score 1-5>\n",
    "}}\n",
    "\"\"\".strip()\n",
    "\n",
    "# Example usage\n",
    "query = \"how to get access to saturn cloud\"\n",
    "answer, contexts = rag(query)  # your RAG pipeline\n",
    "eval_prompt = build_eval_prompt(query, answer, contexts)\n",
    "\n",
    "evaluation = llm(eval_prompt)\n",
    "\n",
    "# Parse JSON safely\n",
    "try:\n",
    "    scores = json.loads(evaluation)\n",
    "    print(\"Evaluation Scores:\", scores)\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Raw LLM Output:\", evaluation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
